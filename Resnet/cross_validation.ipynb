{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"JFF8zNkaPIwo"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/master_thesis/Resnet\n","!pip install carbontracker"]},{"cell_type":"code","source":["import os\n","import sys\n","import json\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import transforms, datasets\n","from tqdm import tqdm\n","\n","from model import resnet34\n","from carbontracker.tracker import CarbonTracker\n","from sklearn.model_selection import StratifiedKFold"],"metadata":{"id":"mlrEUA2M75xa"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ef5qH8uyjAis"},"outputs":[],"source":["def evaluate_model_on_testset(model, device, test_path):\n","    # Assume the test data is structured in a similar way to the training data\n","    test_transform = transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ])\n","    test_dataset = datasets.ImageFolder(root=test_path, transform=test_transform)\n","    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n","\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    test_loss = 0\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    accuracy = 100 * correct / total\n","    print(f'Accuracy on the test set: {accuracy:.2f}%')\n","\n","\n","\n","\n","def exponential_moving_average(data, alpha):\n","    \"\"\"Calculate the exponential moving average using the specified alpha value.\"\"\"\n","    return pd.Series(data).ewm(alpha=alpha, adjust=False).mean().values\n","\n","def plot_and_save_training_results(train_loss_values, val_loss_values, train_accuracy_values, val_accuracy_values, save_path, filename):\n","    if len(train_loss_values) == 0 or len(val_loss_values) == 0:\n","        print(\"No data to plot.\")\n","        return\n","\n","    # Calculate smoothed data\n","    smoothed_train_loss = exponential_moving_average(train_loss_values, 0.5)\n","    smoothed_val_loss = exponential_moving_average(val_loss_values, 0.5)\n","    smoothed_train_accuracy = exponential_moving_average(train_accuracy_values, 0.5)\n","    smoothed_val_accuracy = exponential_moving_average(val_accuracy_values, 0.5)\n","\n","    epochs_range = range(1, len(train_loss_values) + 1)\n","    plt.figure(figsize=(12, 6))\n","\n","    # Plot training and validation loss\n","    plt.subplot(1, 2, 1)\n","    plt.plot(epochs_range, smoothed_train_loss, label='Training Loss', color='blue')\n","    plt.plot(epochs_range, smoothed_val_loss, label='Validation Loss', color='red')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.title('Training and Validation Loss')\n","    plt.legend()\n","\n","    # Plot training and validation accuracy\n","    plt.subplot(1, 2, 2)\n","    plt.plot(epochs_range, smoothed_train_accuracy, label='Training Accuracy', color='blue')\n","    plt.plot(epochs_range, smoothed_val_accuracy, label='Validation Accuracy', color='red')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.title('Training and Validation Accuracy')\n","    plt.legend()\n","    plt.ylim([0, 1])\n","\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(save_path, filename))\n","    plt.show()\n","\n","\n","def count_parameters(model, trainable_only=True):\n","    \"\"\"Calculate the total number of parameters in the model. If trainable_only is True, only count the trainable parameters.\"\"\"\n","    if trainable_only:\n","        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    else:\n","        return sum(p.numel() for p in model.parameters())\n"]},{"cell_type":"code","source":["class NewFC(nn.Module):\n","    def __init__(self, in_features):\n","        super(NewFC, self).__init__()\n","        self.flatten = nn.Flatten()  # Flatten layer\n","        self.fc1 = nn.Linear(in_features, 512)  # Fully connected layer from in_features to 512 features\n","        self.relu = nn.ReLU()  # Activation function\n","        self.dropout = nn.Dropout(0.5)  # Dropout with a rate of 50%\n","        self.fc2 = nn.Linear(512, 2)  # Final layer mapping to 2 classes\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        return x\n"],"metadata":{"id":"CLfjD9DWDis5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Cross-validation of fine-tuning or freezing CNN layers**"],"metadata":{"id":"5Sh9wmvc_LX5"}},{"cell_type":"code","source":["def main():\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    print(\"using {} device.\".format(device))\n","\n","    data_transform = {\n","        \"train\": transforms.Compose([\n","            transforms.RandomResizedCrop(224),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ])\n","    }\n","\n","    image_path = \"/content/drive/MyDrive/master_thesis/Datasets/Herlev\"  # Update this path to your dataset location\n","    assert os.path.exists(image_path), f\"{image_path} path does not exist.\"\n","\n","    dataset = datasets.ImageFolder(root=image_path, transform=data_transform['train'])\n","    targets = [s[1] for s in dataset.samples]  # Extract class labels\n","\n","    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","    fold_performance = []\n","\n","    num_epochs = 200\n","\n","    for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(targets)), targets)):\n","        print(f\"Training fold {fold+1}\")\n","        train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n","        val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n","\n","        train_loader = torch.utils.data.DataLoader(dataset, batch_size=16, sampler=train_sampler)\n","        validate_loader = torch.utils.data.DataLoader(dataset, batch_size=16, sampler=val_sampler)\n","\n","        # net = resnet34(pretrained=True)  # Make sure to adapt this if your model differs\n","        net = resnet34()\n","        # Transfer learning part\n","        model_weight_path = \"./resnet34-pre.pth\"\n","        assert os.path.exists(model_weight_path), \"file {} does not exist.\".format(model_weight_path)\n","        net.load_state_dict(torch.load(model_weight_path, map_location='cpu'))\n","\n","        # Freezing Layer1\n","        for name, param in net.named_parameters():\n","            if \"layer1\" in name:\n","              param.requires_grad = False\n","\n","        # Freezing Layer1 and Layer2\n","        # for name, param in net.named_parameters():\n","        #     if \"layer1\" in name or \"layer2\" in name:\n","        #         param.requires_grad = False\n","\n","        # Freezing Layer1, Layer2, and Layer3\n","        # for name, param in net.named_parameters():\n","        #     if \"layer1\" in name or \"layer2\" in name or \"layer3\" in name:\n","        #         param.requires_grad = False\n","\n","        # Freezing Layer1, Layer2, Layer3, and Layer4\n","        # for name, param in net.named_parameters():\n","        #     if \"layer1\" in name or \"layer2\" in name or \"layer3\" in name or \"layer4\" in name:\n","        #         param.requires_grad = False\n","\n","        # Freezing the entire CNN\n","        # for param in net.parameters():\n","        #     param.requires_grad = False\n","\n","        ######\n","        # Structure A: Basic ResNet34 with modified output layer (choose one structure at a time)\n","        in_channel = net.fc.in_features\n","        net.fc = nn.Linear(in_channel, 2)  # Output layer changed to classify 2 classes\n","        net.to(device)\n","\n","        ######\n","        # Structure B: Add a new fully connected layer without replacing the existing one\n","        # Uncomment the following block if you want to test Structure B:\n","        # in_features = net.fc.in_features\n","        # net.fc = nn.Sequential(\n","        #     nn.Linear(in_features, 1000),  # Original fully connected layer\n","        #     nn.ReLU(),\n","        #     nn.Linear(1000, 2)  # New layer from 1000 classes to 2 classes\n","        # )\n","        # net.to(device)\n","\n","        ######\n","        # Structure C: Replace the original fully connected layer with a new one\n","        # Uncomment the following block if you want to test Structure C:\n","        # in_features = net.fc.in_features\n","        # net.fc = NewFC(in_features)  # Replace with custom fully connected layer\n","        # net.to(device)\n","\n","        total_params = count_parameters(net, trainable_only=False)\n","        trainable_params = count_parameters(net, trainable_only=True)\n","        print(f\"Total parameters: {total_params}\")\n","        print(f\"Trainable parameters: {trainable_params}\")\n","\n","        criterion = nn.CrossEntropyLoss()\n","        optimizer = optim.Adam([p for p in net.parameters() if p.requires_grad], lr=0.0001)\n","\n","        # Train and Validate\n","        train_acc, val_acc = train_validate(net, train_loader, validate_loader, criterion, optimizer, device, num_epochs)\n","        fold_performance.append((train_acc, val_acc))\n","\n","    # Calculate mean and standard deviation for accuracies\n","    train_accs = [fp[0] for fp in fold_performance]\n","    val_accs = [fp[1] for fp in fold_performance]\n","    train_mean = np.mean(train_accs)\n","    train_std = np.std(train_accs)\n","    val_mean = np.mean(val_accs)\n","    val_std = np.std(val_accs)\n","\n","    print(f\"Train Accuracy: Mean = {train_mean:.4f}, Std Dev = {train_std:.4f}\")\n","    print(f\"Validation Accuracy: Mean = {val_mean:.4f}, Std Dev = {val_std:.4f}\")\n","\n","def train_validate(net, train_loader, validate_loader, criterion, optimizer, device, num_epochs=10):\n","    net.train()\n","    for epoch in range(num_epochs):  # Add epoch loop\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = net(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        epoch_loss = running_loss / len(train_loader)\n","        epoch_acc = correct / total\n","        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_acc:.4f}')\n","\n","    # Validate after all training epochs\n","    net.eval()\n","    val_loss = 0.0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for images, labels in validate_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = net(images)\n","            val_loss += criterion(outputs, labels).item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    val_loss /= len(validate_loader)\n","    val_acc = correct / total\n","    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n","\n","    return epoch_acc, val_acc\n","\n","\n","if __name__ == '__main__':\n","    main()\n"],"metadata":{"id":"A61cGsYalWDF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Cross-validation of progressive learning methods**"],"metadata":{"id":"IlcWtAqlKzpa"}},{"cell_type":"code","source":["import os\n","import json\n","import numpy as np\n","import sys\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import transforms, datasets\n","from tqdm import tqdm\n","from sklearn.model_selection import StratifiedKFold\n","\n","from model import resnet34  # Ensure you have a resnet34 or change to the model you are using\n","\n","def main():\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    print(\"using {} device.\".format(device))\n","\n","    data_transform = {\n","        \"train\": transforms.Compose([\n","            transforms.RandomResizedCrop(224),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ])\n","    }\n","\n","    image_path = \"/content/drive/MyDrive/master_thesis/Datasets/Herlev\"\n","    assert os.path.exists(image_path), f\"{image_path} path does not exist.\"\n","\n","    dataset = datasets.ImageFolder(root=image_path, transform=data_transform['train'])\n","    targets = [s[1] for s in dataset.samples]  # extract class labels\n","\n","    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","    fold_performance = []\n","\n","    num_epochs_stage1 = 150\n","    num_epochs_stage2 = 50\n","\n","    for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(targets)), targets)):\n","        print(f\"Training fold {fold+1}\")\n","        train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n","        val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n","\n","        train_loader = torch.utils.data.DataLoader(dataset, batch_size=16, sampler=train_sampler)\n","        validate_loader = torch.utils.data.DataLoader(dataset, batch_size=16, sampler=val_sampler)\n","\n","        # Initialize the model\n","        net = resnet34()\n","        model_weight_path = \"./resnet34-pre.pth\"\n","        assert os.path.exists(model_weight_path), \"file {} does not exist.\".format(model_weight_path)\n","        net.load_state_dict(torch.load(model_weight_path, map_location='cpu', weights_only=True))\n","\n","        # Stage 1: Freeze layer1 and layer2\n","        #for name, param in net.named_parameters():\n","        #    if \"layer1\" in name:\n","        #        param.requires_grad = False\n","\n","        for name, param in net.named_parameters():\n","            if \"layer1\" in name or \"layer2\" in name:\n","                param.requires_grad = False\n","\n","        # Replace the fully connected layer\n","        in_features = net.fc.in_features\n","        net.fc = nn.Sequential(\n","            nn.Linear(in_features, 1000),\n","            nn.ReLU(),\n","            nn.Linear(1000, 2)\n","        )\n","        net.to(device)\n","        #in_features = net.fc.in_features\n","        #net.fc = NewFC(in_features)\n","        #net.to(device)\n","\n","        criterion = nn.CrossEntropyLoss()\n","        optimizer = optim.Adam([p for p in net.parameters() if p.requires_grad], lr=0.0001)\n","\n","\n","\n","        # Train Stage 1\n","        print(f\"Training Stage 1 for fold {fold+1}\")\n","        train_model(net, train_loader, criterion, optimizer, device, num_epochs_stage1)\n","\n","        # Stage 2: Unfreeze layer1 and layer2 for fine-tuning\n","        #for name, param in net.named_parameters():\n","        #    if \"layer1\" in name:\n","        #        param.requires_grad = True\n","\n","        for name, param in net.named_parameters():\n","            if \"layer1\" in name or \"layer2\" in name:\n","               param.requires_grad = True\n","\n","        # Fine-tuning with a smaller learning rate\n","        optimizer = optim.Adam(net.parameters(), lr=1e-5)\n","\n","\n","        # Train Stage 2\n","        print(f\"Fine-tuning Stage 2 for fold {fold+1}\")\n","        train_model(net, train_loader, criterion, optimizer, device, num_epochs_stage2)\n","\n","        # Perform validation after both training stages\n","        print(f\"Validating after 200 epochs for fold {fold+1}\")\n","        val_acc = validate_model(net, validate_loader, criterion, device)\n","        fold_performance.append((None, val_acc))  # No separate train accuracy, only val accuracy after full 200 epochs\n","\n","    # Calculate mean and standard deviation for accuracies\n","    val_accs = [fp[1] for fp in fold_performance]\n","    val_mean = np.mean(val_accs)\n","    val_std = np.std(val_accs)\n","\n","    print(f\"Validation Accuracy: Mean = {val_mean:.4f}, Std Dev = {val_std:.4f}\")\n","\n","def train_model(net, train_loader, criterion, optimizer, device, num_epochs):\n","    net.train()\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = net(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        epoch_loss = running_loss / len(train_loader)\n","        epoch_acc = correct / total\n","        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_acc:.4f}')\n","\n","def validate_model(net, validate_loader, criterion, device):\n","    net.eval()\n","    val_loss = 0.0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for images, labels in validate_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = net(images)\n","            val_loss += criterion(outputs, labels).item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    val_loss /= len(validate_loader)\n","    val_acc = correct / total\n","    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n","\n","    return val_acc\n","\n","if __name__ == '__main__':\n","    main()\n"],"metadata":{"id":"dYIxhtrXHCPU"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyP14Ein/tmR4D0y36NR/hXW"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}